---
version: '3.7'

services:

  elasticsearch:
    image: "amazon/opendistro-for-elasticsearch:{{ opendistro_version }}"
    environment:
      - "ES_JAVA_OPTS=-Xms{{ elasticsearch_memory }}m -Xmx{{ elasticsearch_memory }}m -Djdk.tls.disabledAlgorithms=\"{{ elastic_disabled_tls_protocols|join(',') }}\""
      - "ELASTIC_CLUSTER_NAME=${ELASTIC_CLUSTER_NAME}"
    configs:
      - source: elasticsearch.yml
        target: /usr/share/elasticsearch/config/elasticsearch.yml
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 30s
        max_attempts: 5
      resources:
        limits:
          memory: "{{ elasticsearch_memory }}M"
        reservations:
          memory: "{{ elasticsearch_memory_reservation }}M"
{% if enable_elasticsearch_webui %}
      labels:
        - "traefik.frontend.rule=Host:{{ elasticsearch_domain }}"
        - traefik.enable=true
        - traefik.port=9200
        - "traefik.tags={{ traefik_public_tag }}"
        - "traefik.docker.network={{ traefik_network }}"
        # Traefik service that listens to HTTP
        - traefik.redirectorservice.frontend.entryPoints=http
        - traefik.redirectorservice.frontend.redirect.entryPoint=https
        # Traefik service that listens to HTTPS
        - traefik.webservice.frontend.entryPoints=https
        # security labels
        - traefik.frontend.headers.browserXSSFilter=true
        - traefik.frontend.headers.frameDeny=true
        - traefik.frontend.headers.contentTypeNosniff=true
        # HSTS
        - traefik.frontend.headers.SSLRedirect=true
        - "traefik.frontend.headers.SSLHost={{ elasticsearch_domain }}"
        - traefik.frontend.headers.STSSeconds=315360000
        - traefik.frontend.headers.STSIncludeSubdomains=true
        - traefik.frontend.headers.STSPreload=true
        # Auth
        - "traefik.frontend.auth.basic.users={{ backend_users|join(',') | replace('$', '$$') }}"
        - "traefik.frontend.whiteList.sourceRange={{ trusted_backend_ips|join(',') }},127.0.0.1"
{% endif %}
    networks:
      - stack
{% if enable_elasticsearch_webui %}
      - {{ traefik_network }}
{% endif %}
    volumes:
      - 'es_data:/usr/share/elasticsearch/data'
    healthcheck:
      test: curl -s http://localhost:9200 >/dev/null; if [[ $$? == 52 ]]; then echo 0; else echo 1; fi
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s

  kibana:
    image: "amazon/opendistro-for-elasticsearch-kibana:{{ opendistro_version }}"
    entrypoint: >
      sh -c '
        /usr/share/kibana/bin/kibana-plugin remove opendistro_security &&
        /usr/local/bin/kibana-docker
      '
    configs:
      - source: kibana.yml
        target: /usr/share/kibana/config/kibana.yml
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        delay: 30s
        max_attempts: 4
        window: 120s
      labels:
        - "traefik.frontend.rule=Host:{{ kibana_domain }}"
        - traefik.enable=true
        - traefik.port=5601
        - "traefik.tags={{ traefik_public_tag }}"
        - "traefik.docker.network={{ traefik_network }}"
        # Traefik service that listens to HTTP
        - traefik.redirectorservice.frontend.entryPoints=http
        - traefik.redirectorservice.frontend.redirect.entryPoint=https
        # Traefik service that listens to HTTPS
        - traefik.webservice.frontend.entryPoints=https
        # security labels
        - traefik.frontend.headers.browserXSSFilter=true
        - traefik.frontend.headers.frameDeny=true
        - traefik.frontend.headers.contentTypeNosniff=true
        # HSTS
        - traefik.frontend.headers.SSLRedirect=true
        - "traefik.frontend.headers.SSLHost={{ kibana_domain }}"
        - traefik.frontend.headers.STSSeconds=315360000
        - traefik.frontend.headers.STSIncludeSubdomains=true
        - traefik.frontend.headers.STSPreload=true
        # Auth
        - "traefik.frontend.auth.basic.users={{ backend_users|join(',') | replace('$', '$$') }}"
        - "traefik.frontend.whiteList.sourceRange={{ trusted_backend_ips|join(',') }},127.0.0.1"
    environment:
      - "ELASTIC_CLUSTER_NAME=${ELASTIC_CLUSTER_NAME}"
    networks:
      - stack
      - {{ traefik_network }}
    healthcheck:
      test: curl -s http://localhost:5601 >/dev/null; if [[ $$? == 52 ]]; then echo 0; else echo 1; fi
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s

# forward kibana alerts using smtp
  alerts-smtp-forwarder:
    image: "youtous/odfe-alerts-handler:latest"
    networks:
      - stack
    environment:
      - "SMTP_HOSTNAME=${SMTP_HOSTNAME}"
      - "SMTP_PORT=${SMTP_PORT}"
      - "SMTP_USERNAME=${SMTP_USERNAME}"
      - "SMTP_PASSWORD=${SMTP_PASSWORD}"
      - "SMTP_FROM=${SMTP_FROM}"
    healthcheck:
      interval: 30s
      timeout: 10s
      retries: 5
    deploy:
      mode: replicated
      replicas: 1
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 30s
        max_attempts: 3

  # process logs from everywhere
  logstash:
    image: "docker.elastic.co/logstash/logstash-oss:{{ elastic_version }}"
    configs:
      - source: logstash.yml
        target: /usr/share/logstash/config/logstash.yml
      - source: pipelines.yml
        target: /usr/share/logstash/config/pipelines.yml
{% for pipeline in logstash_pipeline_files %}
      - source: "pipeline_{{ pipeline }}"
        target: "/usr/share/logstash/pipeline-main/{{ pipeline }}"
{% endfor %}
{% for grok in logstash_pipeline_patterns %}
      - source: "pattern_{{ grok }}"
        target: "/usr/share/logstash/pipeline-main/patterns.d/{{ grok }}"
{% endfor %}
    secrets:
      - source: logstash-rootCA.crt
        target: /etc/ca.crt
      - source: logstash-certificate.crt
        target: /etc/server.crt
      - source: logstash-private-key.key
        target: /etc/server.key
    environment:
      - "LS_JAVA_OPTS=-Xms{{ logstash_memory }}m -Xmx{{ logstash_memory }}m -Djdk.tls.disabledAlgorithms=\"{{ elastic_disabled_tls_protocols|join(',') }}\""
      - "ELASTIC_CLUSTER_NAME=${ELASTIC_CLUSTER_NAME}"
    ports: # in order to receive logs from allowed origins, theses ports are opened, access are filtered using UFW
      - target: 5000 # tcp
        published: 5000
        mode: host
      - target: 5044 # beats
        published: 5044
        mode: host
      - target: 5064 # external input
        published: 5064
        mode: host
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 30s
        max_attempts: 3
      resources:
        limits:
          memory: "{{ logstash_memory }}M"
{% if enable_logstash_webui %}
      labels:
        - "traefik.frontend.rule=Host:{{ logstash_domain }}"
        - traefik.enable=true
        - traefik.port=8080
        - "traefik.tags={{ traefik_public_tag }}"
        - "traefik.docker.network={{ traefik_network }}"
        # Traefik service that listens to HTTP
        - traefik.redirectorservice.frontend.entryPoints=http
        - traefik.redirectorservice.frontend.redirect.entryPoint=https
        # Traefik service that listens to HTTPS
        - traefik.webservice.frontend.entryPoints=https
        # security labels
        - traefik.frontend.headers.browserXSSFilter=true
        - traefik.frontend.headers.frameDeny=true
        - traefik.frontend.headers.contentTypeNosniff=true
        # HSTS
        - traefik.frontend.headers.SSLRedirect=true
        - "traefik.frontend.headers.SSLHost={{ logstash_domain }}"
        - traefik.frontend.headers.STSSeconds=315360000
        - traefik.frontend.headers.STSIncludeSubdomains=true
        - traefik.frontend.headers.STSPreload=true
        # Auth => here we can configure access for other clusters to submit logs
        - "traefik.frontend.auth.basic.users={{ backend_users|join(',') | replace('$', '$$') }}"
        - "traefik.frontend.whiteList.sourceRange={{ trusted_backend_ips|join(',') }},127.0.0.1"
{% endif %}
    networks:
      - stack
{% if enable_logstash_webui %}
      - {{ traefik_network }}
{% endif %}
    healthcheck:
      test: curl -s http://localhost:8082 >/dev/null; if [[ $$? == 52 ]]; then echo 0; else echo 1; fi
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 300s

  journalbeat-setup:
    image: "docker.elastic.co/beats/journalbeat-oss:{{ elastic_version }}"
    entrypoint: >
      sh -c '
      journalbeat setup
      '
    configs:
      - source: journalbeat.yml
        target: /usr/share/journalbeat/journalbeat.yml
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 300s
        max_attempts: 0 # wait elastic and kibana to be up
    networks:
      - stack

  metricbeat-setup:
    image: "docker.elastic.co/beats/metricbeat-oss:{{ elastic_version }}"
    entrypoint: >
      sh -c '
      metricbeat setup
      '
    configs:
      - source: metricbeat.yml
        target: /usr/share/metricbeat/metricbeat.yml
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 300s
        max_attempts: 0 # wait elastic and kibana to be up
    networks:
      - stack

  filebeat-setup:
    image: "docker.elastic.co/beats/filebeat-oss:{{ elastic_version }}"
    entrypoint: >
      sh -c '
      filebeat setup
      '
    configs:
      - source: filebeat.yml
        target: /usr/share/filebeat/filebeat.yml
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 300s
        max_attempts: 0 # wait elastic and kibana to be up
    networks:
      - stack

  # check if services are up
  heartbeat:
    image: "docker.elastic.co/beats/heartbeat-oss:{{ elastic_version }}"
    entrypoint: >
      sh -c '
      heartbeat setup &&
      heartbeat --strict.perms=false -e
      '
    configs: # -e flag to log to stderr and disable syslog/file output
      - source: heartbeat.yml
        target: /usr/share/heartbeat/heartbeat.yml
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      restart_policy:
        condition: on-failure
        delay: 5s
        window: 300s
        max_attempts: 0
      resources:
        limits:
          memory: 64M
    networks:
      - stack
    healthcheck:
      test: heartbeat test config
      interval: 30s
      timeout: 15s
      retries: 5
      start_period: 60s

networks:
  stack: # encrypted if Swarm want to split services on managers
    driver: overlay
    driver_opts:
      encrypted: ""
  {{ traefik_network }}:
    external: true

# use docker volume to persist ES data outside of a container.
volumes:
  es_data: {}

# documentation about keystore https://nicklang.com/posts/learning-to-love-the-keystore
# an other way to store sensible information but in this case, the user of the docker-cluster already have privileged access
configs:
  logstash.yml:
    name: "{{ docker_elastic_stack_name }}_logstash.j2.yml-${DEPLOY_TIMESTAMP}"
    file: ./config/logstash/logstash.j2.yml
  pipelines.yml:
    name: "{{ docker_elastic_stack_name }}_pipelines.j2.yml-${DEPLOY_TIMESTAMP}"
    file: ./config/logstash/pipelines.j2.yml
  elasticsearch.yml:
    name: "{{ docker_elastic_stack_name }}_elasticsearch.j2.yml-${DEPLOY_TIMESTAMP}"
    file: ./config/elasticsearch/elasticsearch.j2.yml
  kibana.yml:
    name: "{{ docker_elastic_stack_name }}_kibana.j2.yml-${DEPLOY_TIMESTAMP}"
    file: ./config/kibana/kibana.j2.yml
  heartbeat.yml:
    name: "{{ docker_elastic_stack_name }}_heartbeat.j2.yml-${DEPLOY_TIMESTAMP}"
    file: ./config/heartbeat/heartbeat.j2.yml
  filebeat.yml:
    name: "{{ docker_elastic_stack_name }}_filebeat.j2.yml-${DEPLOY_TIMESTAMP}"
    file: ./config/filebeat/filebeat.j2.yml
  journalbeat.yml:
    name: "{{ docker_elastic_stack_name }}_journalbeat.j2.yml-${DEPLOY_TIMESTAMP}"
    file: ./config/journalbeat/journalbeat.j2.yml
  metricbeat.yml:
    name: "{{ docker_elastic_stack_name }}_metricbeat.j2.yml-${DEPLOY_TIMESTAMP}"
    file: ./config/metricbeat/metricbeat.j2.yml

{% for pipeline in logstash_pipeline_files %}
  pipeline_{{ pipeline }}:
    name: "{{ docker_elastic_stack_name }}_{{ pipeline }}-${DEPLOY_TIMESTAMP}"
    file: "./config/logstash/pipeline/{{ pipeline }}"
{% endfor %}
{% for grok in logstash_pipeline_patterns %}
  pattern_{{ grok }}:
    name: "{{ docker_elastic_stack_name }}_{{ grok }}-${DEPLOY_TIMESTAMP}"
    file: "./config/logstash/pipeline/patterns.d/{{ grok }}"
{% endfor %}

secrets:
  logstash-rootCA.crt:
    name: "{{ docker_elastic_stack_name }}_logstash-rootCA.crt-${DEPLOY_TIMESTAMP}"
    external: true
  logstash-certificate.crt:
    name: "{{ docker_elastic_stack_name }}_logstash-certificate.crt-${DEPLOY_TIMESTAMP}"
    external: true
  logstash-private-key.key:
    name: "{{ docker_elastic_stack_name }}_logstash-private-key.key-${DEPLOY_TIMESTAMP}"
    external: true
